{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import rospy\n",
    "from utils import ReplayBuffer,MultiStepMemory,PER\n",
    "from PER import Memory\n",
    "from std_msgs.msg import Float32MultiArray\n",
    "import sys\n",
    "import os\n",
    "# from Env.environment_stage_2 import Env\n",
    "import numpy as np\n",
    "log_interval = 5           # print avg reward after interval\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 256            # num of transitions sampled from replay buffer\n",
    "lr = 5e-4\n",
    "exploration_noise =0.8\n",
    "polyak = 0.995              # target policy update parameter (1-tau)\n",
    "policy_noise = 0.1         # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2            # delayed policy updates parameter\n",
    "max_episodes = 600         # max num of episodes\n",
    "max_timesteps = 500        # max timesteps in one episode\n",
    "pretrain_times = 0\n",
    "warmup_epoch = 0\n",
    "state_dim = 64\n",
    "action_dim = 2\n",
    "max_action = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Actor(nn.Module):\n",
    "    \n",
    "#     def __init__(self, state_dim, action_dim, max_action):\n",
    "#         super(Actor, self).__init__()\n",
    "        \n",
    "#         self.l1 = nn.Linear(state_dim, 256)\n",
    "#         self.l2 = nn.Linear(256, 64)\n",
    "#         self.l3 = nn.Linear(64, action_dim)\n",
    "        \n",
    "#         self.max_action = max_action\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         a = F.relu(self.l1(state))\n",
    "#         a = F.relu(self.l2(a))\n",
    "#         a = torch.tanh(self.l3(a)) * self.max_action\n",
    "#         return a\n",
    "        \n",
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(Critic, self).__init__()\n",
    "        \n",
    "#         self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "#         self.l2 = nn.Linear(256, 64)\n",
    "#         self.l3 = nn.Linear(64, 1)\n",
    "        \n",
    "#     def forward(self, state, action):\n",
    "#         state_action = torch.cat([state, action], 1)\n",
    "        \n",
    "#         q = F.relu(self.l1(state_action))\n",
    "#         q = F.relu(self.l2(q))\n",
    "#         q = self.l3(q)\n",
    "#         return q\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(state_dim-4, 24)\n",
    "        # self.l2 = nn.Linear(256, 24)\n",
    "        self.l2 = nn.Linear(24+4,256)\n",
    "        self.l3 = nn.Linear(256,64)\n",
    "        self.l4 = nn.Linear(64, action_dim)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state[:,:-4]))\n",
    "        a = F.relu(self.l2(torch.cat([a,state[:,-4:]],1)))\n",
    "        a = F.relu(self.l3(a))\n",
    "        a = torch.tanh(self.l4(a)) * self.max_action\n",
    "        return a\n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(state_dim-4, 24)\n",
    "        self.l2 = nn.Linear(24+4+action_dim, 256)\n",
    "        self.l3 = nn.Linear(256,64)\n",
    "        self.l4 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        # state_action = torch.cat([state, action], 1)\n",
    "        q = F.relu(self.l1(state[:,:-4]))\n",
    "        q = F.relu(self.l2(torch.cat([q,state[:,-4:],action],1)))\n",
    "        q = F.relu(self.l3(q))\n",
    "        q = self.l4(q)\n",
    "        return q    \n",
    "\n",
    "class TD3:\n",
    "    def __init__(self, lr, state_dim, action_dim, max_action):\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
    "        self.state_dim = state_dim\n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def batch_inference(self,state):\n",
    "        state = torch.FloatTensor(state.reshape(-1, self.state_dim)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def analyze_action(self,state,action):\n",
    "        result1 = self.critic_1(state,action)\n",
    "        result2 = self.critic_2(state,action)\n",
    "        return result1,result2\n",
    "\n",
    "    # def state_estimate(self,state,action):\n",
    "    #     laser = state[:-4]\n",
    "    #     heading = state[-4]\n",
    "    #     dis = state[-3]\n",
    "\n",
    "    #     delta_t = 0.2\n",
    "    #     max_angle_vel = 2\n",
    "    #     max_linear_spd = 0.3\n",
    "    #     vx = action[1] * max_linear_spd/2 + max_linear_spd/2\n",
    "    #     wz = action[0] * max_angle_vel\n",
    "    #     theta = wz * delta_t\n",
    "    #     delta_x = vx * delta_t\n",
    "    #     new_dis = dis - delta_x * math.cos(theta)\n",
    "    #     new_heading = heading - theta\n",
    "\n",
    "    #     if new_heading > math.pi:\n",
    "    #         new_heading -= 2 * math.pi\n",
    "\n",
    "    #     elif new_heading < -math.pi:\n",
    "    #         new_heading += 2 * math.pi\n",
    "\n",
    "    #     return new_dis,new_heading\n",
    "\n",
    "\n",
    "    def actor_SL(self,replay_buffer,n_iter,batch_size,sample_mode=1):\n",
    "        for i in range(n_iter):\n",
    "            # Sample a batch of transitions from replay buffer:\n",
    "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size,sample_mode)\n",
    "            # state, action_, reward, next_state, done,idx,weights = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = torch.FloatTensor(action_).to(device)\n",
    "            action_inference = self.actor(state)\n",
    "            actor_loss = F.mse_loss(action_inference,action).mean()\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            if i == 0:\n",
    "                print(\"actor_loss:%d\",actor_loss.item())\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_( param.data)\n",
    "\n",
    "    def critic_SL(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay,sample_mode=0):\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            # Sample a batch of transitions from replay buffer:\n",
    "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size,sample_mode)\n",
    "            # state, action_, reward, next_state, done,idx,weights = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = torch.FloatTensor(action_).to(device)\n",
    "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
    "            # Select next action according to target policy:\n",
    "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise)\n",
    "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute target Q-value:\n",
    "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
    "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
    "\n",
    "            # Optimize Critic 1:\n",
    "            current_Q1 = self.critic_1(state, action)\n",
    "            # replay_buffer.batch_update(idx,torch.abs(target_Q-current_Q1).detach().cpu().numpy())\n",
    "            # loss_Q1 = (torch.pow(current_Q1-target_Q,2) * torch.FloatTensor(weights).cuda()).mean()\n",
    "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
    "            self.critic_1_optimizer.zero_grad()\n",
    "            loss_Q1.backward()\n",
    "            self.critic_1_optimizer.step()\n",
    "            \n",
    "            # Optimize Critic 2:\n",
    "            current_Q2 = self.critic_2(state, action)\n",
    "            # loss_Q2 = (torch.pow(current_Q2-target_Q,2) * torch.FloatTensor(weights).cuda()).mean()\n",
    "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
    "            self.critic_2_optimizer.zero_grad()\n",
    "            loss_Q2.backward()\n",
    "            self.critic_2_optimizer.step()\n",
    "\n",
    "            if i == 0:\n",
    "                print(\"critic_loss:{}\".format(loss_Q1.item()))\n",
    "            \n",
    "            for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
    "                target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "            \n",
    "            for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
    "                target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "\n",
    "        \n",
    "        \n",
    "      \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IMPORT DATASET WITH LENGTH:20001\n"
     ]
    }
   ],
   "source": [
    "class importDataset(object):\n",
    "    def __init__(self, state_dim, action_dim,max_size=-1):\n",
    "        self.datasetPath = \"/home/cmq/ljn/RL/turtlebot3/src/dqn-navigation/turtlebot3_machine_learning/turtlebot3_machine_learning/scripts/dataset_cali.npy\"\n",
    "        self.dataset = np.load(self.datasetPath, allow_pickle=True)\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def transfer(self):\n",
    "        self.buffer = []\n",
    "        for data in self.dataset:\n",
    "            if len(self.buffer) == self.max_size:\n",
    "                break\n",
    "            state = data[0]\n",
    "            action = data[1]\n",
    "            reward = data[2]\n",
    "            next_state = data[3]\n",
    "            finish = float(data[4])\n",
    "            assert len(state) == state_dim\n",
    "            assert(len(action)) == action_dim\n",
    "            assert len(next_state) == state_dim\n",
    "            self.buffer.append((state, action, reward, next_state, finish))\n",
    "        return self.buffer\n",
    "policy = TD3(lr, state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer()\n",
    "replay_buffer.importDataset(importDataset(state_dim,action_dim).transfer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0.0775, -0.0457])\ntensor([0.5833, 0.0000])\ntensor(0.5218)\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss():\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    sample = replay_buffer.sample(128,1)\n",
    "    action = policy.batch_inference(sample[0])\n",
    "    action = torch.FloatTensor(action).reshape(-1,2)\n",
    "    print(action[1])\n",
    "    gt = torch.FloatTensor(sample[1])\n",
    "    print(gt[1])\n",
    "\n",
    "    loss = loss_fn(action, gt)\n",
    "    print(loss/action.size(0))\n",
    "calculate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('actor supervised learning batch:%d', 0)\n",
      "('actor_loss:%d', 0.2973363995552063)\n",
      "('actor supervised learning batch:%d', 10)\n",
      "('actor_loss:%d', 0.24844270944595337)\n",
      "('actor supervised learning batch:%d', 20)\n",
      "('actor_loss:%d', 0.26869669556617737)\n",
      "('actor supervised learning batch:%d', 30)\n",
      "('actor_loss:%d', 0.23424625396728516)\n",
      "('actor supervised learning batch:%d', 40)\n",
      "('actor_loss:%d', 0.2623080611228943)\n",
      "('actor supervised learning batch:%d', 50)\n",
      "('actor_loss:%d', 0.2664201855659485)\n",
      "('actor supervised learning batch:%d', 60)\n",
      "('actor_loss:%d', 0.24360713362693787)\n",
      "('actor supervised learning batch:%d', 70)\n",
      "('actor_loss:%d', 0.23707878589630127)\n",
      "('actor supervised learning batch:%d', 80)\n",
      "('actor_loss:%d', 0.2589711546897888)\n",
      "('actor supervised learning batch:%d', 90)\n",
      "('actor_loss:%d', 0.2700992822647095)\n",
      "('finish batch :%d', 0)\n",
      "tensor([0.6703, 0.0177])\n",
      "tensor([-0.6667, -0.2334])\n",
      "tensor(0.4984)\n",
      "critic_loss:164.121643066\n",
      "critic_loss:303.043273926\n",
      "critic_loss:143.398986816\n",
      "critic_loss:393.991088867\n",
      "critic_loss:253.397796631\n",
      "critic_loss:349.859680176\n",
      "critic_loss:534.642578125\n",
      "critic_loss:297.85949707\n",
      "critic_loss:291.016357422\n",
      "critic_loss:99.0054931641\n",
      "('finish batch :%d', 10)\n",
      "tensor([0.3850, 0.0058])\n",
      "tensor([1., 0.])\n",
      "tensor(0.4832)\n",
      "critic_loss:108.576469421\n",
      "critic_loss:228.76574707\n",
      "critic_loss:28.2472953796\n",
      "critic_loss:32.6128463745\n",
      "critic_loss:138.665634155\n",
      "critic_loss:230.558624268\n",
      "critic_loss:151.286529541\n",
      "critic_loss:91.2928848267\n",
      "critic_loss:240.366958618\n",
      "critic_loss:93.1786193848\n",
      "('finish batch :%d', 20)\n",
      "tensor([ 0.0167, -0.1849])\n",
      "tensor([ 0.5833, -1.0000])\n",
      "tensor(0.5367)\n",
      "critic_loss:84.500289917\n",
      "critic_loss:102.342025757\n",
      "critic_loss:92.1718444824\n",
      "critic_loss:148.153015137\n",
      "critic_loss:88.6390533447\n",
      "critic_loss:57.4764328003\n",
      "critic_loss:66.9495925903\n",
      "critic_loss:115.422065735\n",
      "critic_loss:94.585647583\n",
      "critic_loss:84.9521102905\n",
      "('finish batch :%d', 30)\n",
      "tensor([ 0.5525, -0.0794])\n",
      "tensor([1., 0.])\n",
      "tensor(0.4764)\n",
      "critic_loss:112.307685852\n",
      "critic_loss:94.1448440552\n",
      "critic_loss:82.6271972656\n",
      "critic_loss:153.798339844\n",
      "critic_loss:110.1745224\n",
      "critic_loss:196.544891357\n",
      "critic_loss:96.3506469727\n",
      "critic_loss:133.896240234\n",
      "critic_loss:127.481704712\n",
      "critic_loss:139.84866333\n",
      "('finish batch :%d', 40)\n",
      "tensor([0.4665, 0.0363])\n",
      "tensor([-0.2500,  0.0965])\n",
      "tensor(0.4962)\n",
      "critic_loss:115.609268188\n",
      "critic_loss:133.700744629\n",
      "critic_loss:110.124984741\n",
      "critic_loss:113.864707947\n",
      "critic_loss:123.332809448\n",
      "critic_loss:137.799743652\n",
      "critic_loss:116.834091187\n",
      "critic_loss:106.010223389\n",
      "critic_loss:106.895973206\n",
      "critic_loss:138.917892456\n",
      "('finish batch :%d', 50)\n",
      "tensor([ 0.2200, -0.0147])\n",
      "tensor([-0.6667,  1.0000])\n",
      "tensor(0.5041)\n",
      "critic_loss:117.295410156\n",
      "critic_loss:88.6999053955\n",
      "critic_loss:139.437438965\n",
      "critic_loss:100.386665344\n",
      "critic_loss:98.7656402588\n",
      "critic_loss:93.4412994385\n",
      "critic_loss:106.507904053\n",
      "critic_loss:146.119598389\n",
      "critic_loss:118.434997559\n",
      "critic_loss:88.9181518555\n",
      "('finish batch :%d', 60)\n",
      "tensor([ 0.3139, -0.0216])\n",
      "tensor([1., 0.])\n",
      "tensor(0.4141)\n",
      "critic_loss:158.948516846\n",
      "critic_loss:91.8820800781\n",
      "critic_loss:113.425827026\n",
      "critic_loss:95.2008209229\n",
      "critic_loss:99.1699447632\n",
      "critic_loss:114.645706177\n",
      "critic_loss:203.979949951\n",
      "critic_loss:183.715118408\n",
      "critic_loss:127.858802795\n",
      "critic_loss:112.672927856\n",
      "('finish batch :%d', 70)\n",
      "tensor([ 0.5167, -0.1143])\n",
      "tensor([-0.2500, -0.2342])\n",
      "tensor(0.4737)\n",
      "critic_loss:96.655090332\n",
      "critic_loss:123.048179626\n",
      "critic_loss:125.422790527\n",
      "critic_loss:128.486373901\n",
      "critic_loss:159.317428589\n",
      "critic_loss:79.6803741455\n",
      "critic_loss:111.527130127\n",
      "critic_loss:146.49029541\n",
      "critic_loss:102.894897461\n",
      "critic_loss:120.902931213\n",
      "('finish batch :%d', 80)\n",
      "tensor([ 0.2257, -0.0320])\n",
      "tensor([-0.2500,  0.0087])\n",
      "tensor(0.5139)\n",
      "critic_loss:132.511306763\n",
      "critic_loss:102.262542725\n",
      "critic_loss:119.696746826\n",
      "critic_loss:120.323562622\n",
      "critic_loss:157.754577637\n",
      "critic_loss:109.09879303\n",
      "critic_loss:154.827606201\n",
      "critic_loss:163.990112305\n",
      "critic_loss:115.58430481\n",
      "critic_loss:104.247947693\n",
      "('finish batch :%d', 90)\n",
      "tensor([0.1208, 0.0137])\n",
      "tensor([ 1.0000, -0.1052])\n",
      "tensor(0.4842)\n",
      "critic_loss:140.361938477\n",
      "critic_loss:117.050964355\n",
      "critic_loss:207.568023682\n",
      "critic_loss:96.9411849976\n",
      "critic_loss:130.049072266\n",
      "critic_loss:153.827026367\n",
      "critic_loss:131.170639038\n",
      "critic_loss:147.358108521\n",
      "critic_loss:193.313568115\n",
      "critic_loss:154.110778809\n"
     ]
    }
   ],
   "source": [
    "    pretrain_times = 100\n",
    "    for i in range(pretrain_times):\n",
    "        if(i%10 == 0):\n",
    "            print(\"actor supervised learning batch:%d\",i)\n",
    "            policy.actor_SL(replay_buffer,500,batch_size)\n",
    "    for i in range(pretrain_times):\n",
    "        if(i % 10 == 0):\n",
    "            print(\"finish batch :%d\", i)\n",
    "            calculate_loss()\n",
    "        policy.critic_SL(replay_buffer, 500, batch_size, gamma,\n",
    "                      polyak, policy_noise, noise_clip, policy_delay, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit ('base': conda)",
   "language": "python",
   "name": "python271664bitbasecondae8ce51124e4f4f5aa4ee8d27fa7884a0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}